{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/june-oh/2024_cseg109/blob/main/LAB2_1_LM_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcN_DHBnbpWX"
      },
      "source": [
        "# LAB2_1_LM_Exercise\n",
        "\n",
        "MLP모델을 이용한 간단한 LM 구현\n",
        "- Sequence 데이터를 다루는 방법\n",
        "- 모델 구현 및 학습\n",
        "- 모델 평가 및 테스트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzP3_KpibpWY"
      },
      "source": [
        "## 데이터 준비에 사용되는 메서드들\n",
        "\n",
        "- `split` 메서드: 문자열을 공백을 기준으로 나누어 리스트로 변환 포함\n",
        "- `set` 메서드: 리스트에서 중복을 제거하여 고유한 단어들로 구성된 집합 생성 포함\n",
        "- `enumerate` 메서드: 리스트의 인덱스와 값을 동시에 반환 포함\n",
        "- `torch.tensor` 메서드: 리스트를 텐서로 변환 포함\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TDe3VJKnDpqo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "text = (\"태양계는 매우 다양하고 흥미로운 천체들로 가득 차 있습니다. 이곳에는 여덟 개의 큰 행성과 다수의 작은 행성들이 있으며, 각각 독특한 특징을 지니고 있습니다. \"\n",
        "        \"예를 들어, 수성은 태양에 가장 가까운 행성으로, 매우 뜨거운 낮과 매우 추운 밤을 경험합니다. 반면에, 목성은 가장 큰 행성으로 강력한 자기장을 가지고 있습니다. \"\n",
        "        \"지구는 이 태양계에서 유일하게 생명이 존재하는 것으로 알려진 행성입니다. 물, 산소, 그리고 적당한 온도 덕분에 다양한 생물들이 이곳에서 살아가고 있습니다. \"\n",
        "        \"인간을 포함한 많은 생명체들이 지구의 자연 환경에 적응하여 살아가고 있죠. 과학자들은 지구 밖의 다른 행성들에서도 생명의 흔적을 찾기 위해 끊임없이 탐사를 계속하고 있습니다. \"\n",
        "        \"우주 탐사선과 망원경을 이용하여 멀리 떨어진 행성들과 별들을 관측하고 있죠. 이러한 연구는 우리가 우주에 대해 더 많이 알 수 있게 해주며, \"\n",
        "        \"언젠가는 다른 행성에서도 생명을 찾을 수 있을 것이라는 희망을 줍니다. 우주는 아직도 많은 비밀을 간직하고 있습니다. 이를 탐구하는 과정은 인간의 호기심을 자극하고, \"\n",
        "        \"과학적 지식의 발전에 기여하고 있습니다. 어쩌면 머지않은 미래에 우리는 우주 여행을 일상처럼 할 수 있게 될지도 모릅니다. 그때가 되면, 우주는 더 이상 먼 곳이 아니라 \"\n",
        "        \"우리 생활의 일부가 될 것입니다.\")\n",
        "\n",
        "#Text는 자동으로 생성된 데이터 Thanks to GPT\n",
        "\n",
        "# 토큰화 및 어휘 사전 생성\n",
        "tokens = text.split()\n",
        "vocab = set(tokens)\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "# 토큰을 인덱스로 변환\n",
        "indexed_tokens = [word_to_idx[word] for word in tokens]\n",
        "\n",
        "# 인덱스를 토큰으로 변환\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
        "\n",
        "# 텐서로 변환\n",
        "tensor_tokens = torch.tensor(indexed_tokens, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "lnpbKrTwbpWZ",
        "outputId": "eea43368-dd92-4f50-f67c-b97957895587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens:\n",
            "['태양계는', '매우', '다양하고', '흥미로운', '천체들로', '가득', '차', '있습니다.', '이곳에는', '여덟', '개의', '큰', '행성과', '다수의', '작은', '행성들이', '있으며,', '각각', '독특한', '특징을', '지니고', '있습니다.', '예를', '들어,', '수성은', '태양에', '가장', '가까운', '행성으로,', '매우', '뜨거운', '낮과', '매우', '추운', '밤을', '경험합니다.', '반면에,', '목성은', '가장', '큰', '행성으로', '강력한', '자기장을', '가지고', '있습니다.', '지구는', '이', '태양계에서', '유일하게', '생명이', '존재하는', '것으로', '알려진', '행성입니다.', '물,', '산소,', '그리고', '적당한', '온도', '덕분에', '다양한', '생물들이', '이곳에서', '살아가고', '있습니다.', '인간을', '포함한', '많은', '생명체들이', '지구의', '자연', '환경에', '적응하여', '살아가고', '있죠.', '과학자들은', '지구', '밖의', '다른', '행성들에서도', '생명의', '흔적을', '찾기', '위해', '끊임없이', '탐사를', '계속하고', '있습니다.', '우주', '탐사선과', '망원경을', '이용하여', '멀리', '떨어진', '행성들과', '별들을', '관측하고', '있죠.', '이러한', '연구는', '우리가', '우주에', '대해', '더', '많이', '알', '수', '있게', '해주며,', '언젠가는', '다른', '행성에서도', '생명을', '찾을', '수', '있을', '것이라는', '희망을', '줍니다.', '우주는', '아직도', '많은', '비밀을', '간직하고', '있습니다.', '이를', '탐구하는', '과정은', '인간의', '호기심을', '자극하고,', '과학적', '지식의', '발전에', '기여하고', '있습니다.', '어쩌면', '머지않은', '미래에', '우리는', '우주', '여행을', '일상처럼', '할', '수', '있게', '될지도', '모릅니다.', '그때가', '되면,', '우주는', '더', '이상', '먼', '곳이', '아니라', '우리', '생활의', '일부가', '될', '것입니다.']\n",
            "=====\n",
            "Vocabulary:\n",
            "{'지니고', '과학자들은', '다양하고', '행성들과', '끊임없이', '것입니다.', '다른', '발전에', '어쩌면', '될', '적당한', '환경에', '떨어진', '언젠가는', '작은', '우리가', '다양한', '할', '경험합니다.', '이곳에는', '가득', '흔적을', '연구는', '탐구하는', '생명을', '이를', '생활의', '우주는', '과정은', '여덟', '태양계에서', '행성과', '적응하여', '살아가고', '멀리', '들어,', '그리고', '많은', '개의', '독특한', '호기심을', '모릅니다.', '이용하여', '알', '예를', '우리', '각각', '것으로', '태양계는', '가지고', '찾기', '희망을', '그때가', '아직도', '찾을', '흥미로운', '유일하게', '자극하고,', '아니라', '알려진', '비밀을', '탐사를', '과학적', '생물들이', '있으며,', '것이라는', '온도', '수성은', '자연', '생명체들이', '계속하고', '산소,', '행성들이', '해주며,', '태양에', '밖의', '반면에,', '밤을', '목성은', '행성으로,', '많이', '인간의', '별들을', '관측하고', '가장', '우주', '큰', '우주에', '포함한', '지구', '가까운', '뜨거운', '차', '이', '행성입니다.', '인간을', '우리는', '줍니다.', '생명이', '생명의', '기여하고', '다수의', '행성으로', '특징을', '있죠.', '더', '이상', '곳이', '강력한', '있습니다.', '행성들에서도', '수', '지구는', '추운', '천체들로', '매우', '망원경을', '자기장을', '이러한', '될지도', '여행을', '지식의', '있을', '낮과', '간직하고', '물,', '위해', '행성에서도', '미래에', '일부가', '머지않은', '지구의', '존재하는', '먼', '탐사선과', '덕분에', '되면,', '이곳에서', '있게', '대해', '일상처럼'}\n",
            "=====\n",
            "Word to Index Mapping:\n",
            "{'지니고': 0, '과학자들은': 1, '다양하고': 2, '행성들과': 3, '끊임없이': 4, '것입니다.': 5, '다른': 6, '발전에': 7, '어쩌면': 8, '될': 9, '적당한': 10, '환경에': 11, '떨어진': 12, '언젠가는': 13, '작은': 14, '우리가': 15, '다양한': 16, '할': 17, '경험합니다.': 18, '이곳에는': 19, '가득': 20, '흔적을': 21, '연구는': 22, '탐구하는': 23, '생명을': 24, '이를': 25, '생활의': 26, '우주는': 27, '과정은': 28, '여덟': 29, '태양계에서': 30, '행성과': 31, '적응하여': 32, '살아가고': 33, '멀리': 34, '들어,': 35, '그리고': 36, '많은': 37, '개의': 38, '독특한': 39, '호기심을': 40, '모릅니다.': 41, '이용하여': 42, '알': 43, '예를': 44, '우리': 45, '각각': 46, '것으로': 47, '태양계는': 48, '가지고': 49, '찾기': 50, '희망을': 51, '그때가': 52, '아직도': 53, '찾을': 54, '흥미로운': 55, '유일하게': 56, '자극하고,': 57, '아니라': 58, '알려진': 59, '비밀을': 60, '탐사를': 61, '과학적': 62, '생물들이': 63, '있으며,': 64, '것이라는': 65, '온도': 66, '수성은': 67, '자연': 68, '생명체들이': 69, '계속하고': 70, '산소,': 71, '행성들이': 72, '해주며,': 73, '태양에': 74, '밖의': 75, '반면에,': 76, '밤을': 77, '목성은': 78, '행성으로,': 79, '많이': 80, '인간의': 81, '별들을': 82, '관측하고': 83, '가장': 84, '우주': 85, '큰': 86, '우주에': 87, '포함한': 88, '지구': 89, '가까운': 90, '뜨거운': 91, '차': 92, '이': 93, '행성입니다.': 94, '인간을': 95, '우리는': 96, '줍니다.': 97, '생명이': 98, '생명의': 99, '기여하고': 100, '다수의': 101, '행성으로': 102, '특징을': 103, '있죠.': 104, '더': 105, '이상': 106, '곳이': 107, '강력한': 108, '있습니다.': 109, '행성들에서도': 110, '수': 111, '지구는': 112, '추운': 113, '천체들로': 114, '매우': 115, '망원경을': 116, '자기장을': 117, '이러한': 118, '될지도': 119, '여행을': 120, '지식의': 121, '있을': 122, '낮과': 123, '간직하고': 124, '물,': 125, '위해': 126, '행성에서도': 127, '미래에': 128, '일부가': 129, '머지않은': 130, '지구의': 131, '존재하는': 132, '먼': 133, '탐사선과': 134, '덕분에': 135, '되면,': 136, '이곳에서': 137, '있게': 138, '대해': 139, '일상처럼': 140}\n",
            "=====\n",
            "Indexed Tokens:\n",
            "[48, 115, 2, 55, 114, 20, 92, 109, 19, 29, 38, 86, 31, 101, 14, 72, 64, 46, 39, 103, 0, 109, 44, 35, 67, 74, 84, 90, 79, 115, 91, 123, 115, 113, 77, 18, 76, 78, 84, 86, 102, 108, 117, 49, 109, 112, 93, 30, 56, 98, 132, 47, 59, 94, 125, 71, 36, 10, 66, 135, 16, 63, 137, 33, 109, 95, 88, 37, 69, 131, 68, 11, 32, 33, 104, 1, 89, 75, 6, 110, 99, 21, 50, 126, 4, 61, 70, 109, 85, 134, 116, 42, 34, 12, 3, 82, 83, 104, 118, 22, 15, 87, 139, 105, 80, 43, 111, 138, 73, 13, 6, 127, 24, 54, 111, 122, 65, 51, 97, 27, 53, 37, 60, 124, 109, 25, 23, 28, 81, 40, 57, 62, 121, 7, 100, 109, 8, 130, 128, 96, 85, 120, 140, 17, 111, 138, 119, 41, 52, 136, 27, 105, 106, 133, 107, 58, 45, 26, 129, 9, 5]\n",
            "=====\n",
            "idx_to_word\n",
            "{0: '지니고', 1: '과학자들은', 2: '다양하고', 3: '행성들과', 4: '끊임없이', 5: '것입니다.', 6: '다른', 7: '발전에', 8: '어쩌면', 9: '될', 10: '적당한', 11: '환경에', 12: '떨어진', 13: '언젠가는', 14: '작은', 15: '우리가', 16: '다양한', 17: '할', 18: '경험합니다.', 19: '이곳에는', 20: '가득', 21: '흔적을', 22: '연구는', 23: '탐구하는', 24: '생명을', 25: '이를', 26: '생활의', 27: '우주는', 28: '과정은', 29: '여덟', 30: '태양계에서', 31: '행성과', 32: '적응하여', 33: '살아가고', 34: '멀리', 35: '들어,', 36: '그리고', 37: '많은', 38: '개의', 39: '독특한', 40: '호기심을', 41: '모릅니다.', 42: '이용하여', 43: '알', 44: '예를', 45: '우리', 46: '각각', 47: '것으로', 48: '태양계는', 49: '가지고', 50: '찾기', 51: '희망을', 52: '그때가', 53: '아직도', 54: '찾을', 55: '흥미로운', 56: '유일하게', 57: '자극하고,', 58: '아니라', 59: '알려진', 60: '비밀을', 61: '탐사를', 62: '과학적', 63: '생물들이', 64: '있으며,', 65: '것이라는', 66: '온도', 67: '수성은', 68: '자연', 69: '생명체들이', 70: '계속하고', 71: '산소,', 72: '행성들이', 73: '해주며,', 74: '태양에', 75: '밖의', 76: '반면에,', 77: '밤을', 78: '목성은', 79: '행성으로,', 80: '많이', 81: '인간의', 82: '별들을', 83: '관측하고', 84: '가장', 85: '우주', 86: '큰', 87: '우주에', 88: '포함한', 89: '지구', 90: '가까운', 91: '뜨거운', 92: '차', 93: '이', 94: '행성입니다.', 95: '인간을', 96: '우리는', 97: '줍니다.', 98: '생명이', 99: '생명의', 100: '기여하고', 101: '다수의', 102: '행성으로', 103: '특징을', 104: '있죠.', 105: '더', 106: '이상', 107: '곳이', 108: '강력한', 109: '있습니다.', 110: '행성들에서도', 111: '수', 112: '지구는', 113: '추운', 114: '천체들로', 115: '매우', 116: '망원경을', 117: '자기장을', 118: '이러한', 119: '될지도', 120: '여행을', 121: '지식의', 122: '있을', 123: '낮과', 124: '간직하고', 125: '물,', 126: '위해', 127: '행성에서도', 128: '미래에', 129: '일부가', 130: '머지않은', 131: '지구의', 132: '존재하는', 133: '먼', 134: '탐사선과', 135: '덕분에', 136: '되면,', 137: '이곳에서', 138: '있게', 139: '대해', 140: '일상처럼'}\n",
            "=====\n",
            "Tensor Tokens:\n",
            "tensor([ 48, 115,   2,  55, 114,  20,  92, 109,  19,  29,  38,  86,  31, 101,\n",
            "         14,  72,  64,  46,  39, 103,   0, 109,  44,  35,  67,  74,  84,  90,\n",
            "         79, 115,  91, 123, 115, 113,  77,  18,  76,  78,  84,  86, 102, 108,\n",
            "        117,  49, 109, 112,  93,  30,  56,  98, 132,  47,  59,  94, 125,  71,\n",
            "         36,  10,  66, 135,  16,  63, 137,  33, 109,  95,  88,  37,  69, 131,\n",
            "         68,  11,  32,  33, 104,   1,  89,  75,   6, 110,  99,  21,  50, 126,\n",
            "          4,  61,  70, 109,  85, 134, 116,  42,  34,  12,   3,  82,  83, 104,\n",
            "        118,  22,  15,  87, 139, 105,  80,  43, 111, 138,  73,  13,   6, 127,\n",
            "         24,  54, 111, 122,  65,  51,  97,  27,  53,  37,  60, 124, 109,  25,\n",
            "         23,  28,  81,  40,  57,  62, 121,   7, 100, 109,   8, 130, 128,  96,\n",
            "         85, 120, 140,  17, 111, 138, 119,  41,  52, 136,  27, 105, 106, 133,\n",
            "        107,  58,  45,  26, 129,   9,   5])\n"
          ]
        }
      ],
      "source": [
        "# 토큰 리스트 출력\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "print(\"=====\")\n",
        "# 어휘 사전 출력\n",
        "print(\"Vocabulary:\")\n",
        "print(vocab)\n",
        "print(\"=====\")\n",
        "# 단어와 인덱스 매핑 출력\n",
        "print(\"Word to Index Mapping:\")\n",
        "print(word_to_idx)\n",
        "print(\"=====\")\n",
        "# 인덱스로 변환된 토큰 출력\n",
        "print(\"Indexed Tokens:\")\n",
        "print(indexed_tokens)\n",
        "print(\"=====\")\n",
        "print(\"idx_to_word\")\n",
        "print(idx_to_word)\n",
        "print(\"=====\")\n",
        "# 텐서로 변환된 토큰 출력\n",
        "print(\"Tensor Tokens:\")\n",
        "print(tensor_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fxQ2NsHbpWa"
      },
      "source": [
        "## `nn.Embedding`\n",
        "\n",
        "- `nn.Embedding`은 단어를 고정된 크기의 벡터로 변환하는 데 사용\n",
        "- 임베딩 레이어는 단어의 인덱스를 입력으로 받아 해당 인덱스에 대한 임베딩 벡터를 반환\n",
        "- 임베딩 벡터는 학습 가능한 파라미터로 초기화된 후, 학습 과정에서 업데이트\n",
        "- `nn.Embedding` arguments:\n",
        "  - `num_embeddings`: 임베딩할 단어의 개수 (어휘 사전의 크기)\n",
        "  - `embedding_dim`: 각 단어를 임베딩할 벡터의 차원\n",
        "- PyTorch 공식 문서에서 `nn.Embedding`에 대한 자세한 내용을 확인할 수 있음: [nn.Embedding Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "\n",
        "\n",
        "\n",
        "### 예제 코드\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 임베딩 레이어 생성\n",
        "embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=3)\n",
        "\n",
        "# 임베딩 벡터 출력\n",
        "input_indices = torch.tensor([1, 2, 3, 4])\n",
        "embedding_vectors = embedding_layer(input_indices)\n",
        "print(embedding_vectors)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PHABqXVIbpWa",
        "outputId": "ca0593cd-a2e5-4bec-e3a3-4574cb91efbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding vector of the first token:\n",
            "tensor([ 0.8347, -0.0455, -1.1539], grad_fn=<EmbeddingBackward0>)\n",
            "Shape of the embedding vector of the first token:\n",
            "torch.Size([3])\n",
            "=====\n",
            "Shape of the entire token tensor:\n",
            "torch.Size([161])\n",
            "Shape after passing the entire token tensor through the embedding layer:\n",
            "torch.Size([161, 3])\n"
          ]
        }
      ],
      "source": [
        "# nn.Embedding example\n",
        "embedding_layer = nn.Embedding(num_embeddings=len(tensor_tokens), embedding_dim=3)\n",
        "# Print the embedding vector of the first token\n",
        "print(\"Embedding vector of the first token:\")\n",
        "print(embedding_layer(tensor_tokens[0]))\n",
        "\n",
        "# Print the shape of the embedding vector of the first token\n",
        "print(\"Shape of the embedding vector of the first token:\")\n",
        "print(embedding_layer(tensor_tokens[0]).shape)\n",
        "print(\"=====\")\n",
        "\n",
        "# Print the shape of the entire token tensor\n",
        "print(\"Shape of the entire token tensor:\")\n",
        "print(tensor_tokens.shape)\n",
        "\n",
        "# Print the shape after passing the entire token tensor through the embedding layer\n",
        "print(\"Shape after passing the entire token tensor through the embedding layer:\")\n",
        "print(embedding_layer(tensor_tokens).shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iag_zE5sMNMw"
      },
      "source": [
        "UnigramLM은 앞의 한단어만을 보고 다음 단어를 예측하도록 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset 클래스를 정의\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, word_to_idx):\n",
        "        self.text = text\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.data = [word_to_idx[word] for word in text.split()]\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of data\n",
        "        # fill it\n",
        "        return\n",
        "    def __getitem__(self, idx):\n",
        "        # return context, target\n",
        "        # fill it\n",
        "        return\n",
        "\n",
        "# 데이터셋 인스턴스 생성\n",
        "\n",
        "\n",
        "dataset = TextDataset(text, word_to_idx)\n",
        "\n",
        "# DataLoader를 사용하여 배치 데이터 생성\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([48])\n",
            "태양계는\n",
            "tensor([115])\n",
            "매우\n"
          ]
        }
      ],
      "source": [
        "context, target = next(iter(dataloader))\n",
        "print(context)\n",
        "print(idx_to_word[context[0].item()])\n",
        "print(target)\n",
        "print(idx_to_word[target[0].item()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PdfhVptLDsdO"
      },
      "outputs": [],
      "source": [
        "class UnigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(UnigramLanguageModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        #적절한 layer를 추가하세요.\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        #forward 함수를 작성하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR0E0XckEoRT",
        "outputId": "6d0870b6-bf0b-4486-91a7-d0246764ea5c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'corpus' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m UnigramLanguageModel(vocab_size, embedding_dim)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 학습 데이터 준비\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 각 토큰에 대해 다음 토큰을 예측하는 작업을 수행합니다.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# train_data와 target_data를 준비\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m train_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([word_to_idx[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcorpus\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     10\u001b[0m target_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([word_to_idx[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m corpus[\u001b[38;5;241m1\u001b[39m:]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     12\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
          ]
        }
      ],
      "source": [
        "# 모델 인스턴스 생성\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 128  # 예시로 설정한 임베딩 차원\n",
        "model = UnigramLanguageModel(vocab_size, embedding_dim)\n",
        "\n",
        "# 학습 데이터 준비\n",
        "# 각 토큰에 대해 다음 토큰을 예측하는 작업을 수행합니다.\n",
        "# 각각을 작성하세요\n",
        "train_data =\n",
        "target_data =\n",
        "\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for i in range(len(train_data)):\n",
        "        model.zero_grad()\n",
        "        output = model(train_data[i].unsqueeze(0))\n",
        "        loss = loss_function(output, target_data[i].unsqueeze(0))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if epoch % 10 == 0:\n",
        "      print(f'Epoch {epoch}: Total Loss {total_loss/len(train_data)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk65HXmrErd1"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(model, start_token, word_to_idx, idx_to_word, max_length=20):\n",
        "    \"\"\"\n",
        "    문장 생성 함수\n",
        "    :param model: 학습된 모델\n",
        "    :param start_token: 문장 생성을 시작할 첫 단어\n",
        "    :param word_to_ix: 단어를 인덱스로 변환하는 사전\n",
        "    :param ix_to_word: 인덱스를 단어로 변환하는 사전\n",
        "    :param max_length: 생성할 문장의 최대 길이\n",
        "    :return: 생성된 문장\n",
        "    \"\"\"\n",
        "    current_word = start_token\n",
        "    sentence = [current_word]\n",
        "    model.eval()  # 모델을 추론 모드로 설정\n",
        "\n",
        "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
        "        for _ in range(max_length - 1):\n",
        "            #auto regressive 하게 입력이 바뀌도록 작성하세요.\n",
        "            #mystery = 현재입력\n",
        "            mystery =\n",
        "            token_index = torch.tensor([mystery], dtype=torch.long)\n",
        "            output = model(token_index)\n",
        "            next_word_ix = output.argmax(1).item()\n",
        "            next_word = ix_to_word[next_word_ix]\n",
        "\n",
        "            sentence.append(next_word)\n",
        "            current_word = next_word\n",
        "\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "# 인덱스를 단어로 변환하는 사전 생성\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow0GhiT-Heyx",
        "outputId": "96d07cf4-a945-4e31-91db-14de229a0b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'것으로', '천체들로', '태양계에서', '있습니다.', '지구는', '존재하는', '생명을', '연구는', '밤을', '우리가', '행성들에서도', '알', '강력한', '독특한', '이러한', '수', '위해', '있을', '다양하고', '될', '산소,', '알려진', '인간을', '물,', '발전에', '가까운', '다양한', '가지고', '행성으로,', '자극하고,', '큰', '적응하여', '먼', '탐사선과', '행성으로', '적당한', '행성들과', '생명이', '낮과', '이를', '지식의', '대해', '어쩌면', '아니라', '멀리', '작은', '끊임없이', '곳이', '아직도', '할', '기여하고', '일상처럼', '다수의', '탐구하는', '여행을', '언젠가는', '것이라는', '추운', '유일하게', '별들을', '우리는', '흥미로운', '과학적', '반면에,', '포함한', '흔적을', '많이', '특징을', '태양계는', '매우', '예를', '살아가고', '관측하고', '태양에', '환경에', '각각', '행성들이', '여덟', '행성과', '이곳에는', '되면,', '찾을', '우주는', '희망을', '자기장을', '것입니다.', '생명의', '인간의', '호기심을', '머지않은', '있으며,', '망원경을', '우주', '이상', '그때가', '자연', '많은', '생물들이', '이곳에서', '가장', '더', '차', '생활의', '줍니다.', '행성에서도', '그리고', '모릅니다.', '미래에', '들어,', '개의', '비밀을', '밖의', '지니고', '과정은', '이', '해주며,', '찾기', '지구의', '떨어진', '있게', '뜨거운', '탐사를', '일부가', '온도', '생명체들이', '계속하고', '목성은', '간직하고', '될지도', '행성입니다.', '다른', '우리', '경험합니다.', '수성은', '있죠.', '지구', '가득', '이용하여', '과학자들은', '덕분에', '우주에'}\n"
          ]
        }
      ],
      "source": [
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHBMejlsGab6",
        "outputId": "5efe1337-3f5c-4f4d-c0e9-453af20a7031"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "강력한 자기장을 가지고 있습니다. 어쩌면 머지않은 미래에 우리는 우주 여행을 일상처럼 할 수 있게 될지도 모릅니다. 그때가 되면, 우주는 더\n"
          ]
        }
      ],
      "source": [
        "start_token = '강력한'  # 시작 단어 설정\n",
        "generated_sentence = generate_sentence(model, start_token, word_to_idx, idx_to_word)\n",
        "print(generated_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J-VUh0nGdM3",
        "outputId": "c73aa8a4-66b1-4540-d480-e583e9698e8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "인간의 호기심을 자극하고, 과학적 지식의 발전에 기여하고 있습니다. 어쩌면 머지않은 미래에 우리는 우주 여행을 일상처럼 할 수 있게 될지도 모릅니다.\n"
          ]
        }
      ],
      "source": [
        "start_token = '인간의'  # 시작 단어 설정\n",
        "generated_sentence = generate_sentence(model, start_token, word_to_idx, idx_to_word)\n",
        "print(generated_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzPdtXucGgiZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
