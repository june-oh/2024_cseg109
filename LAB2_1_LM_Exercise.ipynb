{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/june-oh/cseg109/blob/main/Lab2_LM_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LAB2_1_LM_Exercise\n",
        "\n",
        "MLP모델을 이용한 간단한 LM 구현\n",
        "- Sequence 데이터를 다루는 방법\n",
        "- 모델 구현 및 학습\n",
        "- 모델 평가 및 테스트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 데이터 준비에 사용되는 메서드들\n",
        "\n",
        "- `split` 메서드: 문자열을 공백을 기준으로 나누어 리스트로 변환 포함\n",
        "- `set` 메서드: 리스트에서 중복을 제거하여 고유한 단어들로 구성된 집합 생성 포함\n",
        "- `enumerate` 메서드: 리스트의 인덱스와 값을 동시에 반환 포함\n",
        "- `torch.tensor` 메서드: 리스트를 텐서로 변환 포함\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TDe3VJKnDpqo"
      },
      "outputs": [],
      "source": [
        "import torch  \n",
        "import torch.nn as nn  \n",
        "import torch.optim as optim \n",
        "from torch.utils.data import Dataset, DataLoader  \n",
        "\n",
        "text = (\"태양계는 매우 다양하고 흥미로운 천체들로 가득 차 있습니다. 이곳에는 여덟 개의 큰 행성과 다수의 작은 행성들이 있으며, 각각 독특한 특징을 지니고 있습니다. \"\n",
        "        \"예를 들어, 수성은 태양에 가장 가까운 행성으로, 매우 뜨거운 낮과 매우 추운 밤을 경험합니다. 반면에, 목성은 가장 큰 행성으로 강력한 자기장을 가지고 있습니다. \"\n",
        "        \"지구는 이 태양계에서 유일하게 생명이 존재하는 것으로 알려진 행성입니다. 물, 산소, 그리고 적당한 온도 덕분에 다양한 생물들이 이곳에서 살아가고 있습니다. \"\n",
        "        \"인간을 포함한 많은 생명체들이 지구의 자연 환경에 적응하여 살아가고 있죠. 과학자들은 지구 밖의 다른 행성들에서도 생명의 흔적을 찾기 위해 끊임없이 탐사를 계속하고 있습니다. \"\n",
        "        \"우주 탐사선과 망원경을 이용하여 멀리 떨어진 행성들과 별들을 관측하고 있죠. 이러한 연구는 우리가 우주에 대해 더 많이 알 수 있게 해주며, \"\n",
        "        \"언젠가는 다른 행성에서도 생명을 찾을 수 있을 것이라는 희망을 줍니다. 우주는 아직도 많은 비밀을 간직하고 있습니다. 이를 탐구하는 과정은 인간의 호기심을 자극하고, \"\n",
        "        \"과학적 지식의 발전에 기여하고 있습니다. 어쩌면 머지않은 미래에 우리는 우주 여행을 일상처럼 할 수 있게 될지도 모릅니다. 그때가 되면, 우주는 더 이상 먼 곳이 아니라 \"\n",
        "        \"우리 생활의 일부가 될 것입니다.\")\n",
        "\n",
        "#Text는 자동으로 생성된 데이터 Thanks to GPT\n",
        "\n",
        "# 토큰화 및 어휘 사전 생성\n",
        "tokens = text.split()\n",
        "vocab = set(tokens)\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "# 토큰을 인덱스로 변환\n",
        "indexed_tokens = [word_to_idx[word] for word in tokens]\n",
        "\n",
        "# 텐서로 변환\n",
        "tensor_tokens = torch.tensor(indexed_tokens, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens:\n",
            "=====\n",
            "['태양계는', '매우', '다양하고', '흥미로운', '천체들로', '가득', '차', '있습니다.', '이곳에는', '여덟', '개의', '큰', '행성과', '다수의', '작은', '행성들이', '있으며,', '각각', '독특한', '특징을', '지니고', '있습니다.', '예를', '들어,', '수성은', '태양에', '가장', '가까운', '행성으로,', '매우', '뜨거운', '낮과', '매우', '추운', '밤을', '경험합니다.', '반면에,', '목성은', '가장', '큰', '행성으로', '강력한', '자기장을', '가지고', '있습니다.', '지구는', '이', '태양계에서', '유일하게', '생명이', '존재하는', '것으로', '알려진', '행성입니다.', '물,', '산소,', '그리고', '적당한', '온도', '덕분에', '다양한', '생물들이', '이곳에서', '살아가고', '있습니다.', '인간을', '포함한', '많은', '생명체들이', '지구의', '자연', '환경에', '적응하여', '살아가고', '있죠.', '과학자들은', '지구', '밖의', '다른', '행성들에서도', '생명의', '흔적을', '찾기', '위해', '끊임없이', '탐사를', '계속하고', '있습니다.', '우주', '탐사선과', '망원경을', '이용하여', '멀리', '떨어진', '행성들과', '별들을', '관측하고', '있죠.', '이러한', '연구는', '우리가', '우주에', '대해', '더', '많이', '알', '수', '있게', '해주며,', '언젠가는', '다른', '행성에서도', '생명을', '찾을', '수', '있을', '것이라는', '희망을', '줍니다.', '우주는', '아직도', '많은', '비밀을', '간직하고', '있습니다.', '이를', '탐구하는', '과정은', '인간의', '호기심을', '자극하고,', '과학적', '지식의', '발전에', '기여하고', '있습니다.', '어쩌면', '머지않은', '미래에', '우리는', '우주', '여행을', '일상처럼', '할', '수', '있게', '될지도', '모릅니다.', '그때가', '되면,', '우주는', '더', '이상', '먼', '곳이', '아니라', '우리', '생활의', '일부가', '될', '것입니다.']\n",
            "Vocabulary:\n",
            "=====\n",
            "{'지니고', '과학자들은', '다양하고', '행성들과', '끊임없이', '것입니다.', '다른', '발전에', '어쩌면', '될', '적당한', '환경에', '떨어진', '언젠가는', '작은', '우리가', '다양한', '할', '경험합니다.', '이곳에는', '가득', '흔적을', '연구는', '탐구하는', '생명을', '이를', '생활의', '우주는', '과정은', '여덟', '태양계에서', '행성과', '적응하여', '살아가고', '멀리', '들어,', '그리고', '많은', '개의', '독특한', '호기심을', '모릅니다.', '이용하여', '알', '예를', '우리', '각각', '것으로', '태양계는', '가지고', '찾기', '희망을', '그때가', '아직도', '찾을', '흥미로운', '유일하게', '자극하고,', '아니라', '알려진', '비밀을', '탐사를', '과학적', '생물들이', '있으며,', '것이라는', '온도', '수성은', '자연', '생명체들이', '계속하고', '산소,', '행성들이', '해주며,', '태양에', '밖의', '반면에,', '밤을', '목성은', '행성으로,', '많이', '인간의', '별들을', '관측하고', '가장', '우주', '큰', '우주에', '포함한', '지구', '가까운', '뜨거운', '차', '이', '행성입니다.', '인간을', '우리는', '줍니다.', '생명이', '생명의', '기여하고', '다수의', '행성으로', '특징을', '있죠.', '더', '이상', '곳이', '강력한', '있습니다.', '행성들에서도', '수', '지구는', '추운', '천체들로', '매우', '망원경을', '자기장을', '이러한', '될지도', '여행을', '지식의', '있을', '낮과', '간직하고', '물,', '위해', '행성에서도', '미래에', '일부가', '머지않은', '지구의', '존재하는', '먼', '탐사선과', '덕분에', '되면,', '이곳에서', '있게', '대해', '일상처럼'}\n",
            "Word to Index Mapping:\n",
            "=====\n",
            "{'지니고': 0, '과학자들은': 1, '다양하고': 2, '행성들과': 3, '끊임없이': 4, '것입니다.': 5, '다른': 6, '발전에': 7, '어쩌면': 8, '될': 9, '적당한': 10, '환경에': 11, '떨어진': 12, '언젠가는': 13, '작은': 14, '우리가': 15, '다양한': 16, '할': 17, '경험합니다.': 18, '이곳에는': 19, '가득': 20, '흔적을': 21, '연구는': 22, '탐구하는': 23, '생명을': 24, '이를': 25, '생활의': 26, '우주는': 27, '과정은': 28, '여덟': 29, '태양계에서': 30, '행성과': 31, '적응하여': 32, '살아가고': 33, '멀리': 34, '들어,': 35, '그리고': 36, '많은': 37, '개의': 38, '독특한': 39, '호기심을': 40, '모릅니다.': 41, '이용하여': 42, '알': 43, '예를': 44, '우리': 45, '각각': 46, '것으로': 47, '태양계는': 48, '가지고': 49, '찾기': 50, '희망을': 51, '그때가': 52, '아직도': 53, '찾을': 54, '흥미로운': 55, '유일하게': 56, '자극하고,': 57, '아니라': 58, '알려진': 59, '비밀을': 60, '탐사를': 61, '과학적': 62, '생물들이': 63, '있으며,': 64, '것이라는': 65, '온도': 66, '수성은': 67, '자연': 68, '생명체들이': 69, '계속하고': 70, '산소,': 71, '행성들이': 72, '해주며,': 73, '태양에': 74, '밖의': 75, '반면에,': 76, '밤을': 77, '목성은': 78, '행성으로,': 79, '많이': 80, '인간의': 81, '별들을': 82, '관측하고': 83, '가장': 84, '우주': 85, '큰': 86, '우주에': 87, '포함한': 88, '지구': 89, '가까운': 90, '뜨거운': 91, '차': 92, '이': 93, '행성입니다.': 94, '인간을': 95, '우리는': 96, '줍니다.': 97, '생명이': 98, '생명의': 99, '기여하고': 100, '다수의': 101, '행성으로': 102, '특징을': 103, '있죠.': 104, '더': 105, '이상': 106, '곳이': 107, '강력한': 108, '있습니다.': 109, '행성들에서도': 110, '수': 111, '지구는': 112, '추운': 113, '천체들로': 114, '매우': 115, '망원경을': 116, '자기장을': 117, '이러한': 118, '될지도': 119, '여행을': 120, '지식의': 121, '있을': 122, '낮과': 123, '간직하고': 124, '물,': 125, '위해': 126, '행성에서도': 127, '미래에': 128, '일부가': 129, '머지않은': 130, '지구의': 131, '존재하는': 132, '먼': 133, '탐사선과': 134, '덕분에': 135, '되면,': 136, '이곳에서': 137, '있게': 138, '대해': 139, '일상처럼': 140}\n",
            "Indexed Tokens:\n",
            "=====\n",
            "[48, 115, 2, 55, 114, 20, 92, 109, 19, 29, 38, 86, 31, 101, 14, 72, 64, 46, 39, 103, 0, 109, 44, 35, 67, 74, 84, 90, 79, 115, 91, 123, 115, 113, 77, 18, 76, 78, 84, 86, 102, 108, 117, 49, 109, 112, 93, 30, 56, 98, 132, 47, 59, 94, 125, 71, 36, 10, 66, 135, 16, 63, 137, 33, 109, 95, 88, 37, 69, 131, 68, 11, 32, 33, 104, 1, 89, 75, 6, 110, 99, 21, 50, 126, 4, 61, 70, 109, 85, 134, 116, 42, 34, 12, 3, 82, 83, 104, 118, 22, 15, 87, 139, 105, 80, 43, 111, 138, 73, 13, 6, 127, 24, 54, 111, 122, 65, 51, 97, 27, 53, 37, 60, 124, 109, 25, 23, 28, 81, 40, 57, 62, 121, 7, 100, 109, 8, 130, 128, 96, 85, 120, 140, 17, 111, 138, 119, 41, 52, 136, 27, 105, 106, 133, 107, 58, 45, 26, 129, 9, 5]\n",
            "Tensor Tokens:\n",
            "=====\n",
            "tensor([ 48, 115,   2,  55, 114,  20,  92, 109,  19,  29,  38,  86,  31, 101,\n",
            "         14,  72,  64,  46,  39, 103,   0, 109,  44,  35,  67,  74,  84,  90,\n",
            "         79, 115,  91, 123, 115, 113,  77,  18,  76,  78,  84,  86, 102, 108,\n",
            "        117,  49, 109, 112,  93,  30,  56,  98, 132,  47,  59,  94, 125,  71,\n",
            "         36,  10,  66, 135,  16,  63, 137,  33, 109,  95,  88,  37,  69, 131,\n",
            "         68,  11,  32,  33, 104,   1,  89,  75,   6, 110,  99,  21,  50, 126,\n",
            "          4,  61,  70, 109,  85, 134, 116,  42,  34,  12,   3,  82,  83, 104,\n",
            "        118,  22,  15,  87, 139, 105,  80,  43, 111, 138,  73,  13,   6, 127,\n",
            "         24,  54, 111, 122,  65,  51,  97,  27,  53,  37,  60, 124, 109,  25,\n",
            "         23,  28,  81,  40,  57,  62, 121,   7, 100, 109,   8, 130, 128,  96,\n",
            "         85, 120, 140,  17, 111, 138, 119,  41,  52, 136,  27, 105, 106, 133,\n",
            "        107,  58,  45,  26, 129,   9,   5])\n"
          ]
        }
      ],
      "source": [
        "# 토큰 리스트 출력\n",
        "print(\"Tokens:\")\n",
        "print(\"=====\")\n",
        "print(tokens)\n",
        "\n",
        "# 어휘 사전 출력\n",
        "print(\"Vocabulary:\")\n",
        "print(\"=====\")\n",
        "print(vocab)\n",
        "\n",
        "# 단어와 인덱스 매핑 출력\n",
        "print(\"Word to Index Mapping:\")\n",
        "print(\"=====\")\n",
        "print(word_to_idx)\n",
        "\n",
        "# 인덱스로 변환된 토큰 출력\n",
        "print(\"Indexed Tokens:\")\n",
        "print(\"=====\")\n",
        "print(indexed_tokens)\n",
        "\n",
        "# 텐서로 변환된 토큰 출력\n",
        "print(\"Tensor Tokens:\")\n",
        "print(\"=====\")\n",
        "print(tensor_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `nn.Embedding`\n",
        "\n",
        "- `nn.Embedding`은 단어를 고정된 크기의 벡터로 변환하는 데 사용\n",
        "- 임베딩 레이어는 단어의 인덱스를 입력으로 받아 해당 인덱스에 대한 임베딩 벡터를 반환\n",
        "- 임베딩 벡터는 학습 가능한 파라미터로 초기화된 후, 학습 과정에서 업데이트\n",
        "- `nn.Embedding` arguments:\n",
        "  - `num_embeddings`: 임베딩할 단어의 개수 (어휘 사전의 크기)\n",
        "  - `embedding_dim`: 각 단어를 임베딩할 벡터의 차원\n",
        "- PyTorch 공식 문서에서 `nn.Embedding`에 대한 자세한 내용을 확인할 수 있음: [nn.Embedding Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "\n",
        "\n",
        "\n",
        "### 예제 코드\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 임베딩 레이어 생성\n",
        "embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=3)\n",
        "\n",
        "# 임베딩 벡터 출력\n",
        "input_indices = torch.tensor([1, 2, 3, 4])\n",
        "embedding_vectors = embedding_layer(input_indices)\n",
        "print(embedding_vectors)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding vector of the first token:\n",
            "tensor([-0.8147, -0.7424, -0.9069], grad_fn=<EmbeddingBackward0>)\n",
            "Shape of the embedding vector of the first token:\n",
            "torch.Size([3])\n",
            "=====\n",
            "Shape of the entire token tensor:\n",
            "torch.Size([161])\n",
            "Shape after passing the entire token tensor through the embedding layer:\n",
            "torch.Size([161, 3])\n"
          ]
        }
      ],
      "source": [
        "# nn.Embedding example\n",
        "embedding_layer = nn.Embedding(num_embeddings=len(tensor_tokens), embedding_dim=3)\n",
        "# Print the embedding vector of the first token\n",
        "print(\"Embedding vector of the first token:\")\n",
        "print(embedding_layer(tensor_tokens[0]))\n",
        "\n",
        "# Print the shape of the embedding vector of the first token\n",
        "print(\"Shape of the embedding vector of the first token:\")\n",
        "print(embedding_layer(tensor_tokens[0]).shape)\n",
        "print(\"=====\")\n",
        "\n",
        "# Print the shape of the entire token tensor\n",
        "print(\"Shape of the entire token tensor:\")\n",
        "print(tensor_tokens.shape)\n",
        "\n",
        "# Print the shape after passing the entire token tensor through the embedding layer\n",
        "print(\"Shape after passing the entire token tensor through the embedding layer:\")\n",
        "print(embedding_layer(tensor_tokens).shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iag_zE5sMNMw"
      },
      "source": [
        "UnigramLM은 앞의 한단어만을 보고 다음 단어를 예측하도록 작성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "PdfhVptLDsdO"
      },
      "outputs": [],
      "source": [
        "class UnigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(UnigramLanguageModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        #적절한 layer를 추가하세요.\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        #forward 함수를 작성하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR0E0XckEoRT",
        "outputId": "6d0870b6-bf0b-4486-91a7-d0246764ea5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Total Loss 5.148554638028145\n"
          ]
        }
      ],
      "source": [
        "# 모델 인스턴스 생성\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 128  # 예시로 설정한 임베딩 차원\n",
        "model = UnigramLanguageModel(vocab_size, embedding_dim)\n",
        "\n",
        "# 학습 데이터 준비\n",
        "# 각 토큰에 대해 다음 토큰을 예측하는 작업을 수행합니다.\n",
        "# 각각을 작성하세요\n",
        "train_data =\n",
        "target_data =\n",
        "\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for i in range(len(train_data)):\n",
        "        model.zero_grad()\n",
        "        output = model(train_data[i].unsqueeze(0))\n",
        "        loss = loss_function(output, target_data[i].unsqueeze(0))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if epoch % 10 == 0:\n",
        "      print(f'Epoch {epoch}: Total Loss {total_loss/len(train_data)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "rk65HXmrErd1"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(model, start_token, word_to_idx, idx_to_word, max_length=20):\n",
        "    \"\"\"\n",
        "    문장 생성 함수\n",
        "    :param model: 학습된 모델\n",
        "    :param start_token: 문장 생성을 시작할 첫 단어\n",
        "    :param word_to_ix: 단어를 인덱스로 변환하는 사전\n",
        "    :param ix_to_word: 인덱스를 단어로 변환하는 사전\n",
        "    :param max_length: 생성할 문장의 최대 길이\n",
        "    :return: 생성된 문장\n",
        "    \"\"\"\n",
        "    current_word = start_token\n",
        "    sentence = [current_word]\n",
        "    model.eval()  # 모델을 추론 모드로 설정\n",
        "\n",
        "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
        "        for _ in range(max_length - 1):\n",
        "            #auto regressive 하게 입력이 바뀌도록 작성하세요.\n",
        "            #mystery = 현재입력\n",
        "            mystery =\n",
        "            token_index = torch.tensor([mystery], dtype=torch.long)\n",
        "            output = model(token_index)\n",
        "            next_word_ix = output.argmax(1).item()\n",
        "            next_word = ix_to_word[next_word_ix]\n",
        "\n",
        "            sentence.append(next_word)\n",
        "            current_word = next_word\n",
        "\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "# 인덱스를 단어로 변환하는 사전 생성\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow0GhiT-Heyx",
        "outputId": "96d07cf4-a945-4e31-91db-14de229a0b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'것으로', '천체들로', '태양계에서', '있습니다.', '지구는', '존재하는', '생명을', '연구는', '밤을', '우리가', '행성들에서도', '알', '강력한', '독특한', '이러한', '수', '위해', '있을', '다양하고', '될', '산소,', '알려진', '인간을', '물,', '발전에', '가까운', '다양한', '가지고', '행성으로,', '자극하고,', '큰', '적응하여', '먼', '탐사선과', '행성으로', '적당한', '행성들과', '생명이', '낮과', '이를', '지식의', '대해', '어쩌면', '아니라', '멀리', '작은', '끊임없이', '곳이', '아직도', '할', '기여하고', '일상처럼', '다수의', '탐구하는', '여행을', '언젠가는', '것이라는', '추운', '유일하게', '별들을', '우리는', '흥미로운', '과학적', '반면에,', '포함한', '흔적을', '많이', '특징을', '태양계는', '매우', '예를', '살아가고', '관측하고', '태양에', '환경에', '각각', '행성들이', '여덟', '행성과', '이곳에는', '되면,', '찾을', '우주는', '희망을', '자기장을', '것입니다.', '생명의', '인간의', '호기심을', '머지않은', '있으며,', '망원경을', '우주', '이상', '그때가', '자연', '많은', '생물들이', '이곳에서', '가장', '더', '차', '생활의', '줍니다.', '행성에서도', '그리고', '모릅니다.', '미래에', '들어,', '개의', '비밀을', '밖의', '지니고', '과정은', '이', '해주며,', '찾기', '지구의', '떨어진', '있게', '뜨거운', '탐사를', '일부가', '온도', '생명체들이', '계속하고', '목성은', '간직하고', '될지도', '행성입니다.', '다른', '우리', '경험합니다.', '수성은', '있죠.', '지구', '가득', '이용하여', '과학자들은', '덕분에', '우주에'}\n"
          ]
        }
      ],
      "source": [
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHBMejlsGab6",
        "outputId": "5efe1337-3f5c-4f4d-c0e9-453af20a7031"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "강력한 자기장을 가지고 있습니다. 어쩌면 머지않은 미래에 우리는 우주 여행을 일상처럼 할 수 있게 될지도 모릅니다. 그때가 되면, 우주는 더\n"
          ]
        }
      ],
      "source": [
        "start_token = '강력한'  # 시작 단어 설정\n",
        "generated_sentence = generate_sentence(model, start_token, word_to_idx, idx_to_word)\n",
        "print(generated_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J-VUh0nGdM3",
        "outputId": "c73aa8a4-66b1-4540-d480-e583e9698e8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "인간의 호기심을 자극하고, 과학적 지식의 발전에 기여하고 있습니다. 어쩌면 머지않은 미래에 우리는 우주 여행을 일상처럼 할 수 있게 될지도 모릅니다.\n"
          ]
        }
      ],
      "source": [
        "start_token = '인간의'  # 시작 단어 설정\n",
        "generated_sentence = generate_sentence(model, start_token, word_to_idx, idx_to_word)\n",
        "print(generated_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzPdtXucGgiZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOyCqNB83rbEP2OjhtFtt8v",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
